# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
# slurm.conf file generated by configurator.html.
# See the slurm.conf man page for more information.

ClusterName=KAT
ControlMachine=sched02
ControlAddr=10.151.0.248

SlurmUser=slurm
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#StateSaveLocation=/home/sched02/tmp
#SlurmdSpoolDir=/home/sched02/tmp/slurmd
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
#MpiDefault=none
MpiDefault=pmi2
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
#ProctrackType=proctrack/linuxproc
ProctrackType=proctrack/cgroup
CacheGroups=0
ReturnToService=0

#Prolog and Epilog
#PrologFlags=Alloc
PrologFlags=contain
Prolog=/home/sched02/cfg/prolog.sh
PrologSlurmctld=/home/sched02/cfg/prolog_ctld.sh
EpilogSlurmctld=/home/sched02/cfg/epilog_ctld.sh
Epilog=/home/sched02/cfg/epilog.sh
TaskProlog=/home/sched02/cfg/task_prolog.sh
TaskEpilog=/home/sched02/cfg/task_epilog.sh
TaskPlugin=task/cgroup
#JobSubmitPlugins=pbs
JobSubmitPlugins=lua

#ulimit
PropagateResourceLimitsExcept=MEMLOCK
#PropagateResourceLimitsExcept=ALL
#PropagateResourceLimits=ALL
PropagateResourceLimits=NONE

#UsePAM=1

# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0

# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/linear
#SelectType=select/cons_res
#SelectTypeParameters=CR_Core
FastSchedule=1

# LOGGING
SlurmctldDebug=verbose
#SlurmctldDebug=7
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=5
SlurmdLogFile=/var/log/slurm/slurmd.log
JobCompType=jobcomp/none
#JobCompLoc=/home/sched02/log/scripts/resultcomp.txt

# Debugging
#DebugFlags=NO_CONF_HASH

# ACCOUNTING
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=30
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=sched02

#AccountingStorageEnforce=associations,qos
AccountingStorageEnforce=limits,qos
AccountingStorageTRES=gres/gpu

# ENERGY
AcctGatherEnergyType=acct_gather_energy/rapl
AcctGatherNodeFreq=30

# COMPUTE NODES
GresTypes=gpu

# COMPUTE NODE
NodeName=gpu[01-08]		Feature="TeslaK40"		CPUs=20 Sockets=2 CoresPerSocket=10 ThreadsPerCore=1 gres=gpu:2 RealMemory=128000
PartitionName=dual_k40_node	Nodes=gpu[01-08]		QOS=dual_k40 OverSubscribe=FORCE Default=NO MaxTime=24:00:00 State=UP AllowAccounts=kat_user,edu_user

NodeName=gpu[09-20]		Feature="TeslaV100-16GB"	CPUs=20 Sockets=2 CoresPerSocket=10 ThreadsPerCore=1 gres=gpu:2 RealMemory=128000
PartitionName=dual_v100_node	Nodes=gpu[09-20]		QOS=dual_v100 OverSubscribe=FORCE Default=NO MaxTime=24:00:00 State=UP AllowAccounts=kat_user,edu_user

NodeName=gpu[21-29]		Feature="TeslaV100-32GB"	CPUs=20 Sockets=2 CoresPerSocket=10 ThreadsPerCore=1 gres=gpu:1 RealMemory=128000
PartitionName=single_v100_node	Nodes=gpu[21-29]		QOS=single_v100 OverSubscribe=FORCE Default=NO MaxTime=24:00:00 State=UP AllowAccounts=kat_user,edu_user

NodeName=skl[01-09]		Feature="Xeon6140 Gold"		CPUs=36 Sockets=2 CoresPerSocket=18 ThreadsPerCore=1 RealMemory=190000
PartitionName=skl_node		Nodes=skl[01-09]		QOS=skl OverSubscribe=FORCE Default=NO MaxTime=24:00:00 State=UP AllowAccounts=kat_user

NodeName=bigmem01		Feature="Xeon Westmere"		Procs=1 CPUs=40 Sockets=4 CoresPerSocket=10 ThreadsPerCore=1 RealMemory=516860 State=UNKNOWN
NodeName=bigmem02		Feature="Xeon Broadwell"	Procs=1 CPUs=56 Sockets=4 CoresPerSocket=14 ThreadsPerCore=1 RealMemory=770000 State=UNKNOWN
PartitionName=bigmem_node 	Nodes=bigmem[01-02]  		QOS=bigmem OverSubscribe=FORCE Default=NO MaxTime=24:00:00 State=UP AllowAccounts=kat_user

##PartitionName=bigmem_node Nodes=bigmem,bigmem2  QOS=bigmem_qos OverSubscribe=FORCE     Default=NO MaxTime=24:00:00 State=UP
